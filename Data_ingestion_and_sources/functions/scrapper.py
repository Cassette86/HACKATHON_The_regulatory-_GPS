#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Scraper "maxi" pour remplir :
- EU.db   : textes principaux EU (tu peux ajouter des URLs)
- USA.db  : toutes les FMVSS (49 CFR Part 571) via ecfr.io
- India.db: catalogue complet des AIS (toutes les pages)
- China.db: URLs Ã  complÃ©ter
- Japan.db: URLs principales MLIT (Ã  complÃ©ter si besoin)

PrÃ©requis :
    pip install requests beautifulsoup4
"""

import sqlite3
import time
import random
from pathlib import Path
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

BASE_DIR = Path(__file__).resolve().parent

DB_PATHS = {
    "EU": BASE_DIR / "EU.db",
    "USA": BASE_DIR / "USA.db",
    "India": BASE_DIR / "India.db",
    "China": BASE_DIR / "China.db",
    "Japan": BASE_DIR / "Japan.db",
    "France": BASE_DIR / "France.db",
    "UK": BASE_DIR / "UK.db",
}


HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/124.0 Safari/537.36"
    )
}

# ---------------------------------------------------------------------------
# 1. Utils BDD
# ---------------------------------------------------------------------------

def init_db():
    """CrÃ©e les tables 'regulations' dans chaque base si besoin."""
    for region, db_path in DB_PATHS.items():
        conn = sqlite3.connect(db_path)
        c = conn.cursor()
        c.execute(
            """
            CREATE TABLE IF NOT EXISTS regulations (
                id         INTEGER PRIMARY KEY AUTOINCREMENT,
                title      TEXT,
                source_url TEXT UNIQUE,
                content    TEXT
            )
            """
        )
        conn.commit()
        conn.close()
        print(f"[INFO] Base initialisÃ©e : {db_path}")


def save_regulation(region: str, title: str, source_url: str, content: str | None):
    """
    Sauvegarde (ou met Ã  jour) une rÃ©gulation dans la base du pays.
    - Si l'URL n'existe pas => INSERT
    - Si l'URL existe mais content vide => UPDATE
    """
    db_path = DB_PATHS[region]
    conn = sqlite3.connect(db_path)
    c = conn.cursor()

    # Insert si pas encore prÃ©sent
    c.execute(
        """
        INSERT OR IGNORE INTO regulations (title, source_url, content)
        VALUES (?, ?, ?)
        """,
        (title, source_url, content),
    )

    # Si dÃ©jÃ  prÃ©sent et qu'on a maintenant du contenu, on met Ã  jour
    if content:
        c.execute(
            """
            UPDATE regulations
            SET content = ?
            WHERE source_url = ?
            AND (content IS NULL OR content = '')
            """,
            (content, source_url),
        )

    conn.commit()
    conn.close()


def fetch_html(url: str, timeout: int = 30) -> str | None:
    """GET simple avec gestion des erreurs et User-Agent."""
    try:
        print(f"[GET] {url}")
        resp = requests.get(url, headers=HEADERS, timeout=timeout)
        resp.raise_for_status()
        return resp.text
    except requests.RequestException as e:
        print(f"[ERREUR] Impossible de rÃ©cupÃ©rer {url} : {e}")
        return None


def scrape_text_page(region: str, url: str, title_hint: str | None = None):
    """
    RÃ©cupÃ¨re tout le texte brut d'une page HTML et l'enregistre dans la DB du pays.
    """
    html = fetch_html(url)
    if not html:
        return

    soup = BeautifulSoup(html, "html.parser")
    page_title = title_hint or (soup.title.get_text(strip=True) if soup.title else url)

    # Texte brut (simple mais efficace pour la recherche plein texte)
    text = soup.get_text("\n", strip=True)

    save_regulation(region, page_title, url, text)
    print(f"[OK] {region} : {page_title} ({len(text)} caractÃ¨res)")
    time.sleep(random.uniform(1.0, 2.5))


# ---------------------------------------------------------------------------
# 2. ðŸ‡ªðŸ‡º Scraper basique EU (liste fixe de gros textes Ã  enrichir)
# ---------------------------------------------------------------------------

EU_URLS = [
    # RÃ¨glement (UE) 2018/858 - type approval vÃ©hicules
    "https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32018R0858",
    # RÃ¨glement (UE) 2019/2144 - General Safety Regulation (ADAS, etc.)
    "https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32019R2144",
    # Tu peux ajouter ici d'autres textes importants si tu veux
    # Type-approval / surveillance du marchÃ© vÃ©hicules lÃ©gers (remplace 2007/46/CE)
    "https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32018R0858",

    # SÃ©curitÃ© gÃ©nÃ©rale & protection usagers vulnÃ©rables
    "https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32019R2144",

    # Ã‰missions Euro 5 / Euro 6 vÃ©hicules lÃ©gers
    "https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32007R0715",

    # Ã‰missions Euro VI vÃ©hicules lourds
    "https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32009R0595",

]


def scrape_eu():
    print("\n================= ðŸ‡ªðŸ‡º Scraping EU =================")
    for url in EU_URLS:
        scrape_text_page("EU", url)


# ---------------------------------------------------------------------------
# 3. ðŸ‡ºðŸ‡¸ FMVSS (49 CFR Part 571) via ecfr.io
# ---------------------------------------------------------------------------

USA_INDEX_URL = "https://ecfr.io/Title-49/Part-571"
USA_SECTION_PREFIX = "/Title-49/Section-571."


USA_URLS = [
    "https://ecfr.io/Title-49/Part-565",  # VIN
    "https://ecfr.io/Title-49/Part-566",  # Manufacturer identification
    "https://ecfr.io/Title-49/Part-567",  # Certification
    "https://ecfr.io/Title-49/Part-568",  # Vehicles built in two or more stages
]


def scrape_usa_fmvss():
    """
    1) RÃ©cupÃ¨re la page index Part 571 sur ecfr.io
    2) Trouve tous les liens /Title-49/Section-571.xxx
    3) TÃ©lÃ©charge chaque section et l'enregistre dans USA.db
    """
    print("\n================= ðŸ‡ºðŸ‡¸ Scraping USA (FMVSS) =================")
    html = fetch_html(USA_INDEX_URL)
    if not html:
        return

    soup = BeautifulSoup(html, "html.parser")

    links = soup.select(f"a[href^='{USA_SECTION_PREFIX}']")
    print(f"[INFO] {len(links)} sections FMVSS trouvÃ©es sur l'index.")

    for a in links:
        href = a.get("href")
        if not href:
            continue

        section_url = urljoin(USA_INDEX_URL, href)
        section_title = a.get_text(" ", strip=True) or section_url

        print(f"[INFO] FMVSS : {section_title}")
        scrape_text_page("USA", section_url, title_hint=section_title)


# ---------------------------------------------------------------------------
# 4. ðŸ‡®ðŸ‡³ AIS â€“ Ministry of Road Transport & Highways (morth.nic.in)
# ---------------------------------------------------------------------------

INDIA_AIS_BASE = "https://morth.nic.in/ais"


def parse_ais_table(soup: BeautifulSoup):
    """
    Dans la page AIS, trouve la table "Automotive Industry Standards (AIS)"
    et enregistre chaque ligne (AIS code + Subject + PDF) dans India.db.
    """
    tables = soup.find_all("table")
    target_table = None

    for table in tables:
        text = table.get_text(" ", strip=True)
        if "Automotive Industry Standards (AIS)" in text:
            target_table = table
            break

    if not target_table:
        print("[WARN] Table AIS non trouvÃ©e sur cette page.")
        return

    tbody = target_table.find("tbody") or target_table
    rows = tbody.find_all("tr")

    count = 0
    for tr in rows:
        tds = tr.find_all("td")
        if len(tds) < 3:
            continue

        # D'aprÃ¨s le site : [NÂ°, AIS Code, Subject, Status, Download, Date]
        cells = [td.get_text(" ", strip=True) for td in tds]
        # On essaie de rester robuste si la structure varie un peu
        try:
            ais_code = cells[1]
            subject = cells[2]
            status = cells[3] if len(cells) > 3 else ""
            date_str = cells[-1]
        except IndexError:
            continue

        pdf_tag = tr.find("a", href=True)
        pdf_url = urljoin(INDIA_AIS_BASE, pdf_tag["href"]) if pdf_tag else INDIA_AIS_BASE

        title = f"{ais_code} - {subject}".strip()

        # On stocke les mÃ©tadonnÃ©es dans 'content' (le PDF reste Ã  parser si tu veux aller plus loin)
        content_lines = [
            f"AIS code: {ais_code}",
            f"Subject: {subject}",
            f"Status: {status}",
            f"Date: {date_str}",
            f"PDF: {pdf_url}",
        ]
        content = "\n".join(content_lines)

        save_regulation("India", title, pdf_url, content)
        count += 1

    print(f"[OK] {count} normes AIS ajoutÃ©es / mises Ã  jour sur cette page.")

# ---------------------------------------------------------------------------
# ðŸ‡«ðŸ‡· France â€“ quelques textes clÃ©s sur Legifrance
# ---------------------------------------------------------------------------

FRANCE_URLS = [
    # Homologation / rÃ©ception des vÃ©hicules (dÃ©jÃ  OK)
    "https://www.ecologie.gouv.fr/politiques-publiques/homologation-vehicules",

    # Politique vÃ©hicules Ã©lectriques (grande page de synthÃ¨se VE)
    "https://www.ecologie.gouv.fr/politiques-publiques/developper-vehicules-electriques",

    # Infrastructures de recharge (trÃ¨s utile pour lâ€™Ã©cosystÃ¨me VE)
    "https://www.ecologie.gouv.fr/politiques-publiques/developpement-nouveaux-equipements-reseaux-recharges-vehicules-electriques",

    # RÃ©trofit Ã©lectrique (conversion thermique -> Ã©lectrique)
    "https://www.ecologie.gouv.fr/politiques-publiques/savoir-retrofit-electrique",

    # Dossier : financer son passage Ã  lâ€™Ã©lectrique
    "https://www.ecologie.gouv.fr/dossiers/savoir-passer-lelectrique/financer-son-passage-lelectrique",
]




def scrape_france():
    print("\n================= ðŸ‡«ðŸ‡· Scraping France (liste fixe) =================")
    for url in FRANCE_URLS:
        scrape_text_page("France", url)


INDIA_URLS = [
    # Page ministÃ¨re qui liste Motor Vehicles Act + CMVR
    "https://parivahan.gov.in/parivahan/en/content/act-rules-and-policies",

    # Fiche officielle CMVR 1989 (MORTH)
    "https://morth.nic.in/en/central-motor-vehicles-rules-1989",
]

def scrape_india_ais():
    """
    Crawl toutes les pages AIS (pagination ?page=N), et enregistre
    chaque entrÃ©e dans India.db.
    """
    print("\n================= ðŸ‡®ðŸ‡³ Scraping India (AIS) =================")

    to_visit = {INDIA_AIS_BASE}
    visited: set[str] = set()

    while to_visit:
        url = to_visit.pop()
        if url in visited:
            continue
        visited.add(url)

        html = fetch_html(url)
        if not html:
            continue

        soup = BeautifulSoup(html, "html.parser")
        print(f"[INFO] Traitement de la page AIS : {url}")
        parse_ais_table(soup)

        # Pagination : on cherche les liens avec '?page=' dans l'URL
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if "ais?page=" in href:
                full = urljoin(url, href)
                if full not in visited:
                    to_visit.add(full)

        # Petite pause
        time.sleep(random.uniform(1.0, 2.0))


# ---------------------------------------------------------------------------
# ðŸ‡¬ðŸ‡§ UK â€“ quelques textes clÃ©s sur legislation.gov.uk_
# ---------------------------------------------------------------------------

UK_URLS = [
    # Vue d'ensemble de la procÃ©dure d'approval (import, vÃ©hicule construit, modifiÃ©, etc.)
    "https://www.gov.uk/vehicle-approval/overview",

    # Page centrale VCA sur la type-approval (GB, UK(NI), UNECE...)
    "https://www.vehicle-certification-agency.gov.uk/vehicle-type-approval/",

    # Explications dÃ©taillÃ©es : "What is Vehicle Type Approval?"
    "https://www.vehicle-certification-agency.gov.uk/vehicle-type-approval/what-is-vehicle-type-approval/",

    # Provisional GB Type Approval scheme (post-Brexit, trÃ¨s intÃ©ressant pour ton use case)
    "https://www.vehicle-certification-agency.gov.uk/vehicle-type-approval/provisional-gb-type-approval-scheme/",
]



def scrape_uk():
    print("\n================= ðŸ‡¬ðŸ‡§ Scraping UK (liste fixe) =================")
    for url in UK_URLS:
        scrape_text_page("UK", url)



# ---------------------------------------------------------------------------
# 5. ðŸ‡¨ðŸ‡³ & ðŸ‡¯ðŸ‡µ â€“ URLs Ã  complÃ©ter (scrape simple)
# ---------------------------------------------------------------------------

CHINA_URLS = [
    # Index gÃ©nÃ©ral des standards "Emission Standard for Mobile-source Pollutants"
    "https://english.mee.gov.cn/Resources/standards/Air_Environment/emission_mobile/",

    # Emissions light-duty vehicles (GB 18352.3-2005 â€“ Euro III/IV like)
    "https://english.mee.gov.cn/Resources/standards/Air_Environment/emission_mobile/200710/t20071024_111848.shtml",

    # China V â€“ Limits and methods for emissions from light-duty vehicles (GB 18352.5-2013)
    "https://english.mee.gov.cn/Resources/standards/Air_Environment/emission_mobile/201605/t20160511_337517.shtml",

    # Hybrid light-duty vehicles (GB 19755-2016)
    "https://english.mee.gov.cn/Resources/standards/Air_Environment/emission_mobile/201609/t20160902_363506.shtml",

    # Bruit â€“ tri-wheel & low-speed vehicle (rÃ©fÃ©rence Ã  GB 7258)
    "https://english.mee.gov.cn/Resources/standards/Noise/Method_standard3/200907/t20090716_156194.shtml",
]


JAPAN_URLS = [
    # Page gÃ©nÃ©rale sur l'inspection des vÃ©hicules (contexte rÃ©glementation)
    "https://www.mlit.go.jp/english/inspect/car09e.html",
    # Exemples de pages "Safety Regulations for Road Vehicles" (appels Ã  commentaires)
    "https://www.mlit.go.jp/english/mot_news/mot_news_990902.html",
    "https://www.mlit.go.jp/english/mot_news/mot_news_000627.html",
    # Tu peux ajouter ici d'autres pages importantes
]


def scrape_china():
    print("\n================= ðŸ‡¨ðŸ‡³ Scraping China (liste fixe) =================")
    for url in CHINA_URLS:
        scrape_text_page("China", url)


def scrape_japan():
    print("\n================= ðŸ‡¯ðŸ‡µ Scraping Japan (liste fixe) =================")
    for url in JAPAN_URLS:
        scrape_text_page("Japan", url)


# ---------------------------------------------------------------------------
# 6. Main
# ---------------------------------------------------------------------------

def main():
    init_db()

    scrape_eu()
    scrape_usa_fmvss()
    scrape_india_ais()
    scrape_china()
    scrape_japan()
    scrape_france()   # <--- ajouter ici
    scrape_uk()       # <--- on ajoute la fonction UK juste aprÃ¨s


    print("\nâœ… Scraping terminÃ©. Tu peux maintenant relancer search_all.py pour tester.")


if __name__ == "__main__":
    main()
